services:
  namenode:
    networks:
      - spark_mts
    image: mirror.gcr.io/apache/hadoop:3.3.6
    hostname: namenode
    user: root
    command: >
      bash -c "
        if [ ! -d /hadoop/dfs/name/current ]; then
          echo 'Formatting NameNode...'
          hdfs namenode -format -force
        fi &&
        hdfs namenode
      "
    ports:
      - 9870:9870
    env_file:
      - ./config.env
    volumes:
      - namenode_data:/hadoop/dfs/name
    environment:
      ENSURE_NAMENODE_DIR: "/hadoop/dfs/name"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode1:
    networks:
      - spark_mts
    ports:
      - 9864:9864
    user: root
    image: mirror.gcr.io/apache/hadoop:3.3.6
    command: ["hdfs", "datanode"]
    volumes:
      - datanode1_data:/hadoop/dfs/data
    env_file:
      - ./config.env
    depends_on:
      namenode:
        condition: service_healthy

  datanode2:
    networks:
      - spark_mts
    ports:
      - 9865:9864  # Different host port
    user: root
    image: mirror.gcr.io/apache/hadoop:3.3.6
    command: ["hdfs", "datanode"]
    volumes:
      - datanode2_data:/hadoop/dfs/data
    env_file:
      - ./config.env
    depends_on:
      namenode:
        condition: service_healthy
    
  hdfs-init:
    image: mirror.gcr.io/apache/hadoop:3.3.6
    hostname: hdfs-init
    networks:
      - spark_mts
    depends_on:
      namenode:
        condition: service_healthy
    env_file:
      - ./config.env
    command: >
      bash -c "
        echo 'Waiting for NameNode...' &&
        sleep 10 &&
        hdfs dfsadmin -safemode wait &&
        echo 'Creating Hive warehouse directories in HDFS...' &&
        hdfs dfs -mkdir -p /user/hive/warehouse &&
        hdfs dfs -chmod -R 777 /user/hive/warehouse &&
        echo 'HDFS initialization complete.'
      "
    restart: "no"

  postgres:
    networks:
      - spark_mts
    image: postgres:11
    hostname: hive-metastore-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_DB=metastore
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive"]
      interval: 10s
      timeout: 5s
      retries: 5

  hiveserver2-standalone:
    user: root
    networks:
      - spark_mts
    image: apache/hive:3.1.3
    depends_on:
      - postgres
    environment:
      SERVICE_NAME: hiveserver2
      DB_DRIVER: postgres
      HIVE_CUSTOM_CONF_DIR: /hive_custom_conf
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=hive              
    volumes:
      - /Users/boldyrevn/Downloads/postgresql-42.7.7.jar:/opt/hive/lib/postgres.jar
      - ./hadoop-config:/hive_custom_conf

  # spark-master:
  #   image: bde2020/spark-master:3.3.0-hadoop3.3
  #   container_name: spark-master
  #   networks:
  #     - spark_mts
  #   ports:
  #     - "8080:8080"
  #     - "7077:7077"
  #   environment:
  #     - INIT_DAEMON_STEP=setup_spark
  #   env_file:
  #     - ./config.env
      
  # spark-worker-1:
  #   image: bde2020/spark-worker:3.3.0-hadoop3.3
  #   container_name: spark-worker-1
  #   networks:
  #     - spark_mts
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     - "SPARK_MASTER=spark://spark-master:7077"
  #     - "SPARK_WORKER_WEBUI_PORT=8081"
  #   env_file:
  #     - ./config.env

  # spark-worker-2:
  #   image: bde2020/spark-worker:3.3.0-hadoop3.3
  #   container_name: spark-worker-2
  #   networks:
  #     - spark_mts
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "8082:8082"
  #   environment:
  #     - "SPARK_MASTER=spark://spark-master:7077"
  #     - "SPARK_WORKER_WEBUI_PORT=8082"
  #   env_file:
  #     - ./config.env

  # spark-history-server:
  #   image: bde2020/spark-history-server:3.3.0-hadoop3.3
  #   container_name: spark-history-server
  #   networks:
  #     - spark_mts
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "18081:18081"
  #   volumes:
  #     - spark-events:/tmp/spark-events
  #   env_file:
  #     - ./config.env

  # jupyter:
  #   build:
  #     context: .
  #     dockerfile: dockerfile.jupyter
  #   networks:
  #     - spark_mts
  #   hostname: jupyter
  #   ports:
  #     - 8888:8888
  #     - 4040:4040
  #     - 4041:4041
  #   environment:
  #     SPARK_MASTER: spark://spark-master:7077
  #     HADOOP_CONF_DIR: /etc/hadoop
  #     HADOOP_USER_NAME: hadoop
  #   volumes:
  #     - ./hadoop-config:/etc/hadoop:ro
  #     - ./notebooks:/home/jovyan/work
  #   depends_on:
  #     - spark-master
  #     - namenode
  #     - hive-metastore
  #     - hiveserver2

networks:
  spark_mts:

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  postgres_data:
  hive_metastore_data:
  hive_server_data:
  spark-events:





